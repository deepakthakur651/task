# -*- coding: utf-8 -*-
"""amazon_url.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r9F5oZ8i9xgUiNZxKbV370_6pKPTUeqE

# Importing libraries of python
"""

import pandas as pd
import numpy as np
import requests
from  bs4 import BeautifulSoup

"""# Uploading dataset """

from google.colab import drive
drive.mount('/content/drive')

data= pd.read_csv('/content/drive/MyDrive/amazon_url/Amazon Scraping - Sheet2.csv')
data=data.set_index('Unnamed: 0')
data.head()

"""# Then create a new variable for use only 100 rows in our dataset.

"""

new_data= data[['Asin','country','url']][0:100]
new_data

"""use only ( url) columns """

web=new_data['url']
web

# importing libraries
from bs4 import BeautifulSoup
import requests
 
def main(URL):
    # opening our output file in append mode
    File = open("out.csv", "a")
 
    # specifying user agent, You can use other user agents
    # available on the internet
    HEADERS = ({'User-Agent':
                'Chrome/44.0.2403.157 Safari/537.36',
                                'Accept-Language': 'en-US, en;q=0.5'})
 
    # Making the HTTP Request
    webpage = requests.get(URL, headers=HEADERS)
 
    # Creating the Soup Object containing all data
    soup = BeautifulSoup(webpage.content, "lxml")
 
    # retrieving product title
    try:
        # Outer Tag Object
        title = soup.find("span",
                          attrs={"id": 'productTitle'})
 
        # Inner NavigableString Object
        title_value = title.string
 
        # Title as a string value
        title_string = title_value.strip().replace(',', '')
 
    except AttributeError:
        title_string = "NA"
    print("product Title = ", title_string)
 
    # saving the title in the file
    File.write(f"{title_string},")
 
    # retrieving price
    try:
        price = soup.find(
            "span", attrs={'id': 'priceblock_ourprice'}).string.strip().replace(',', '')
        # we are omitting unnecessary spaces
        # and commas form our string
    except AttributeError:
        price = "NA"
    print("Products price = ", price)
 
    # saving
    File.write(f"{price},")
 
    # retrieving product rating
    try:
        rating = soup.find("i", attrs={
                           'class': 'a-icon a-icon-star a-star-4-5'}).string.strip().replace(',', '')
 
    except AttributeError:
 
        try:
            rating = soup.find(
                "span", attrs={'class': 'a-icon-alt'}).string.strip().replace(',', '')
        except:
            rating = "NA"
    print("Overall rating = ", rating)
 
    File.write(f"{rating},")
 
    try:
        review_count = soup.find(
            "span", attrs={'id': 'acrCustomerReviewText'}).string.strip().replace(',', '')
 
    except AttributeError:
        review_count = "NA"
    print("Total reviews = ", review_count)
    File.write(f"{review_count},")
 
    # print availablility status
    try:
        available = soup.find("div", attrs={'id': 'availability'})
        available = available.find("span").string.strip().replace(',', '')
 
    except AttributeError:
        available = "NA"
    print("Availability = ", available)
 
    # saving the availability and closing the line
    File.write(f"{available},\n")
 
    # closing the file
    File.close()
 
 

 
    # iterating over the urls
for links in web:
    main(links)

a=[]
b=[]
coun=0
count=0
for i in web:
  if requests.get(i):
    a.append(i)
    coun+=1
  else:
    b.append(i)
    count+=1

print(coun)
print(count)

def amezon_url(url):
  r = requests.get(url)
  soup = BeautifulSoup(r.content, 'html.parser')
  # print(soup.prettify())
  anchor=soup.find_all('a')
  all_links=set()
  for link in anchor:
    if (link !='#'):
      link=link.get('href')
      all_links.add(link)
      print(link)
for link in b:
  amezon_url(i)

"""how to use url in html we can see in our dataset """

from requests.models import Response
def get_html(url):

  Response=requests.get(url)
  print(Response)
  print(Response.content)
  content=Response.text
  print(content)

  soup = BeautifulSoup(url, 'html.parser')
  print(soup.prettify)  
  print(soup.find_all('title'))
a_r='https://www.amazon.de/dp/1015'
get_html(a_r)

"""# Install Library BS4"""

pip install bs4

"""# From BS4 import Beautifulsoup 
# and also import regular expression & requests
"""

from bs4 import BeautifulSoup
import re
import requests

r = requests.get('https://www.amazon.fr/dp/1060619')
 
# check status code for response received
# success code - 200
print(r)
 
# print content of request
print(r.content)
content=r.text
print(content)

soup = BeautifulSoup(r.content, 'html.parser')
# print(soup.prettify())
anchor=soup.find_all('a')
all_links=set()
for link in anchor:
  if (link !='#'):
    link=link.get('href')
    all_links.add(link)
    print(link)

from pandas.core.frame import DataFrame
data1=DataFrame(soup).astype(str)
print(data1)
#print(type(data1))

print(soup.find_all('title'))

pattern = r'(https?:\/\/(?:www\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'

from bs4 import BeautifulSoup

# setting up the URL
URL = 'https://www.amazon.fr/dp/1060619'

# perform get request to the url
reqs = requests.get(URL)

# extract all the text that you received from
# the GET request
content = reqs.text

# convert the text to a beautiful soup object
soup = BeautifulSoup(content, 'html.parser')

# Empty list to store the output
urls = []

# For loop that iterates over all the <li> tags
for h in soup.findAll('img'):
	
	# looking for anchor tag inside the <li>tag
	a = h.find('img')
	try:
		
		# looking for href inside anchor tag
		if 'href' in a.attrs:
			
			# storing the value of href in a separate variable
			url = a.get('href')
			
			# appending the url to the output list
			urls.append(url)
			
	# if the list does not has a anchor tag or an anchor tag
	# does not has a href params we pass
	except:
		pass

# print all the urls stored in the urls list
for url in urls:
	print(url)

import requests
from bs4 import BeautifulSoup as bs
import csv
 
URL = requests.get('https://www.amazon.fr/dp/1060619')
 
soup = bs(URL.text, 'html.parser')
 
titles = soup.find_all('title', attrs={'class', 'head'})
titles_list = []
 
count = 1
for title in titles:
    d = {}
    d['Title Number'] = f'Title {count}'
    d['Title Name'] = title.text
    count += 1
    titles_list.append(d)
 
filename = 'titles.csv'
with open(filename, 'w', newline='') as f:
    w = csv.DictWriter(f,['Title Number','Title Name'])
    w.writeheader()
     
    w.writerows(titles_list)

df=pd.read_csv('/content/titles.csv')
df

import json
import io
from google.colab import files
uploaded = files.upload()

import pandas as pd
import io

io.StringIO(uploaded["Amazon Scraping - Sheet2.csv"].decode("utf-8"))

file_name = "Amazon Scraping - Sheet2.csv"

data = uploaded[file_name].decode("utf-8").split("\r\n")

for i in range(len(data)):
  data[i] = data[i].split(",")

print(data)

pd.read_csv(io.StringIO(uploaded["Amazon Scraping - Sheet2.csv"].decode("utf-8")))





